{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 786
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "12P72HH-8vUm",
    "outputId": "cc019da5-a7d8-4e00-a74c-54bcb6f6f821"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "Requirement already satisfied: openslide-python in /home/bhavya754/anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: Pillow in /home/bhavya754/anaconda3/lib/python3.6/site-packages (from openslide-python)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install the OpenSlide C library and Python bindings\n",
    "!apt-get install openslide-tools\n",
    "!pip install openslide-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "4xUefpmR24U3"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from openslide import open_slide, __library_version__ as openslide_version\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from PIL import Image\n",
    "from skimage.color import rgb2gray\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o3ynjXUhTHgl"
   },
   "source": [
    "# **Generating slide and mask list**\n",
    "\n",
    "We generate our slide and mask filenames list based on the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "9zLahgSJ1z-x",
    "outputId": "7623d337-aab4-4206-d29d-ae451e75c826"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OpenSlide('../vatsala-adl/tumor_101.tif'), OpenSlide('../vatsala-adl/tumor_078.tif')]\n",
      "[OpenSlide('../vatsala-adl/tumor_101_mask.tif'), OpenSlide('../vatsala-adl/tumor_078_mask.tif')]\n"
     ]
    }
   ],
   "source": [
    "# Test slide removed\n",
    "\n",
    "test_image_filename = '../vatsala-adl/tumor_110.tif'\n",
    "test_mask_filename = '../vatsala-adl/tumor_110_mask.tif'\n",
    "\n",
    "# get all filenames for slides and masks not in testing\n",
    "slide_filenames = ['../vatsala-adl/tumor_101.tif','../vatsala-adl/tumor_078.tif']\n",
    "mask_filenames = ['../vatsala-adl/tumor_101_mask.tif','../vatsala-adl/tumor_078_mask.tif']\n",
    "\n",
    "\n",
    "slide_deck = []\n",
    "mask_deck = []\n",
    "\n",
    "# slides\n",
    "for filename in slide_filenames:\n",
    "    slide_deck.append(open_slide(filename))\n",
    "\n",
    "# masks\n",
    "for filename in mask_filenames:\n",
    "    mask_deck.append(open_slide(filename))\n",
    "\n",
    "print(slide_deck)\n",
    "print(mask_deck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "PWLzrJ_m4LTI"
   },
   "outputs": [],
   "source": [
    "def read_slide(slide, x, y, level, width, height, as_float=False):\n",
    "    im = slide.read_region((x,y), level, (width, height))\n",
    "    im = im.convert('RGB') # drop the alpha channel\n",
    "    if as_float:\n",
    "        im = np.asarray(im, dtype=np.float32)\n",
    "    else:\n",
    "        im = np.asarray(im)\n",
    "    assert im.shape == (height, width, 3)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "xVIg9yYxPwMi"
   },
   "outputs": [],
   "source": [
    "# As mentioned in class, we can improve efficiency by ignoring non-tissue areas \n",
    "# of the slide. We'll find these by looking for all gray regions.\n",
    "def find_tissue_pixels(image, intensity=0.8):\n",
    "    im_gray = rgb2gray(image)\n",
    "    assert im_gray.shape == (image.shape[0], image.shape[1])\n",
    "    indices = np.where(im_gray <= intensity)\n",
    "    return zip(indices[0], indices[1])\n",
    "\n",
    "def apply_mask(im, mask, color=(255,0,0)):\n",
    "    masked = np.copy(im)\n",
    "    for x,y in mask: masked[x][y] = color\n",
    "    return masked\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Udexmhnp2vW2"
   },
   "source": [
    "# Preprocessing and Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EEZipPVSUOC6"
   },
   "source": [
    "The first thing we need to do is consider only those subimages which are relevant. Here, based on a threshold value, we ignore the images that mostly are just grey background. To find whether or not an image is a potential background, we calculate the tissue percentage. For this we have created a helper function that returns tissue percentage in each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "VoTI8n3L2zCt"
   },
   "outputs": [],
   "source": [
    "def find_tissue_percentage(slide_region, window_size):\n",
    "    \n",
    "    image_width = window_size\n",
    "    image_height = window_size\n",
    "\n",
    "    def find_tissue_pixels(image, intensity=0.8):\n",
    "        im_gray = rgb2gray(image)\n",
    "        assert im_gray.shape == (image.shape[0], image.shape[1])\n",
    "        indices = np.where(im_gray <= intensity)\n",
    "        return list(zip(indices[0], indices[1]))\n",
    "    \n",
    "    tissue_pixels = find_tissue_pixels(slide_region)\n",
    "    percent_tissue = len(tissue_pixels) / float(image_width * image_height) * 100\n",
    "    \n",
    "    return percent_tissue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below function takes a patch and detects the appropriate label for it (cancerous or not). It does so by taking sum of cancerous pixels (1 denotes cancerous). Basicaly if it finds a cancerous pixel it assigns it label one or cancerous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "1ucYRVbFbIWx"
   },
   "outputs": [],
   "source": [
    "def check_if_cancer(patch_mask, patch_centre):\n",
    "    # detect patch size\n",
    "    patch_size = patch_mask.shape[0]\n",
    "    offset = int((patch_size-patch_centre)/2)\n",
    "    sum_vals = np.sum(patch_mask[offset:offset+patch_centre, offset:offset+patch_centre])\n",
    "    return sum_vals>0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our slide image is very big, we extract smaller patch images from the slide image. We decided to extract maintain separate lists of healthy and tumor images so that we could later on see what happens if we keep a balanced set.\n",
    "\n",
    "We also call the check_if_cancer function internally to detect label for a subimage and find_tissue_percentage to avoid computation overhead in processing majorly gray background\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "HMg0lNcm4SKR"
   },
   "outputs": [],
   "source": [
    "def get_patches_from_images(slide, mask, level, window_size, stride):\n",
    "  \n",
    "    #patch_centre = 128\n",
    "    \n",
    "    tissue_threshold = 0.01\n",
    "    tumor_image_deck = []\n",
    "    tumor_mask_deck = []\n",
    "    tumor_mask_labels = []\n",
    "    healthy_image_deck = []\n",
    "    healthy_mask_deck = []\n",
    "    healthy_mask_labels = []\n",
    "    \n",
    "    #At slide level\n",
    "    image_width = slide.level_dimensions[level][0]\n",
    "    image_height = slide.level_dimensions[level][1]\n",
    "    \n",
    "    x_val = 0\n",
    "    y_val = 0\n",
    "    downsample_factor = int(slide.level_downsamples[level]) #2**level\n",
    "    \n",
    "    # Move window through the slide\n",
    "    while y_val < (image_height - window_size + 1):\n",
    "        print(\"fwd\")\n",
    "        while x_val < (image_width - window_size + 1):\n",
    "            \n",
    "            # slide image\n",
    "            slide_region = read_slide(slide, \n",
    "                         x= x_val * downsample_factor, \n",
    "                         y= y_val * downsample_factor, \n",
    "                         level= level, \n",
    "                         width= window_size, \n",
    "                         height= window_size)\n",
    "            \n",
    "            # mask image\n",
    "            mask_region = read_slide(mask, \n",
    "                         x= x_val * downsample_factor, \n",
    "                         y= y_val * downsample_factor, \n",
    "                         level= level, \n",
    "                         width= window_size, \n",
    "                         height= window_size)\n",
    "            \n",
    "            # one channel enough for mask\n",
    "            mask_region = mask_region[:,:,0]\n",
    "\n",
    "            # find label corresponding to the patch\n",
    "            #reduced from 100 to 25 as per patch size\n",
    "            cancer_present= check_if_cancer(mask_region, 50)\n",
    "            #cancer_present = 1\n",
    "            \n",
    "            # ignore gray images without any tissue\n",
    "            percent_tissue = find_tissue_percentage(slide_region, window_size)\n",
    "\n",
    "            if percent_tissue > tissue_threshold:\n",
    "                if cancer_present:\n",
    "                    tumor_image_deck.append(slide_region)\n",
    "                    tumor_mask_deck.append(mask_region)\n",
    "                    tumor_mask_labels.append(1)\n",
    "                else:\n",
    "                    healthy_image_deck.append(slide_region)\n",
    "                    healthy_mask_deck.append(mask_region)\n",
    "                    healthy_mask_labels.append(0)\n",
    "                  \n",
    "            # move by stride\n",
    "            x_val += stride\n",
    "        \n",
    "        # end of row, reset and move down by stride\n",
    "        x_val = 0\n",
    "        y_val += stride\n",
    "        \n",
    "    return tumor_image_deck, tumor_mask_deck, tumor_mask_labels, healthy_image_deck, healthy_mask_deck, healthy_mask_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aRJLUpD45-1d"
   },
   "source": [
    "## Extraction of patches\n",
    "\n",
    "We run the code below (which extracts the patches) for two levels - 4 , 5. We train on two slides: 101, 78. \n",
    "The data is stored seperately for tumor images and healthy images. This helps us see how skewed the dataset is, and if there is need for data augmentation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "7QyMyPo54cwT",
    "outputId": "ac77779e-2b95-4224-f5ca-9439816ac8bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "1\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "fwd\n",
      "22096 138259\n"
     ]
    }
   ],
   "source": [
    "# taken size at 100 instead of 299, as kept crashing\n",
    "window_size = 150\n",
    "stride = 10\n",
    "level = 4\n",
    "\n",
    "slide_subimages = []\n",
    "slide_submasks = []\n",
    "slide_sublabels = []\n",
    "slide_tumor_subimages = []\n",
    "slide_tumor_submasks = []\n",
    "slide_tumor_sublabels = []\n",
    "slide_healthy_subimages = []\n",
    "slide_healthy_submasks = []\n",
    "slide_healthy_sublabels = []\n",
    "\n",
    "for ind in range(2):\n",
    "    print(ind)\n",
    "    tumor_subimages, tumor_submasks, tumor_sublabels, healthy_subimages, healthy_submasks, healthy_sublabels = get_patches_from_images(slide_deck[ind], mask_deck[ind], level, window_size, stride)\n",
    "    slide_tumor_subimages.extend(tumor_subimages)\n",
    "    slide_tumor_submasks.extend(tumor_submasks)\n",
    "    slide_tumor_sublabels.extend(tumor_sublabels)\n",
    "    slide_healthy_subimages.extend(healthy_subimages)\n",
    "    slide_healthy_submasks.extend(healthy_submasks)\n",
    "    slide_healthy_sublabels.extend(healthy_sublabels)\n",
    "\n",
    "print(len(slide_tumor_subimages), len(slide_healthy_subimages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ca-zQdV6ha6a"
   },
   "source": [
    "# Data Imbalance \n",
    "\n",
    "As we can see, the data is highly skewed. We have far more samples of healthy subimages than tumor images. To remove some of the skewness, we randomly shuffle and drop some of the healthy samples.\n",
    "\n",
    "As we do this for one level, we will also ensure that we drop out the samples for the corresponding zoomed in/zoomed out level. \n",
    "\n",
    "For level 4, we have more than enough data ( >15000 tumor images alone), so we decide to randomly drop some data in order to have a reasonable number of samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "jn7n7SD1q76_",
    "outputId": "1b78111f-3277-4968-dc11-c716b8aa1b67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2209 13825\n",
      "13825\n"
     ]
    }
   ],
   "source": [
    "# slide_subimages.extend(slide_tumor_subimages)\n",
    "# slide_subimages.extend(slide_healthy_subimages)\n",
    "\n",
    "\n",
    "slide_healthy_subimages = np.array(slide_healthy_subimages)\n",
    "slide_tumor_subimages = np.array(slide_tumor_subimages)\n",
    "n_healthy = len(slide_healthy_subimages)\n",
    "n_tumor = len(slide_tumor_subimages)\n",
    "# n_h = min(n_tumor*4, n_healthy//3)\n",
    "\n",
    "randomize = np.arange(n_healthy)\n",
    "np.random.shuffle(randomize)\n",
    "slide_healthy_subimages = slide_healthy_subimages[randomize]\n",
    "slide_healthy_subimages = slide_healthy_subimages[0:n_healthy//10,:]\n",
    "slide_healthy_sublabels = slide_healthy_sublabels[0:n_healthy//10]\n",
    "\n",
    "randomize = np.arange(n_tumor)\n",
    "np.random.shuffle(randomize)\n",
    "slide_tumor_subimages = slide_tumor_subimages[randomize]\n",
    "slide_tumor_subimages = slide_tumor_subimages[0:n_tumor//10,:]\n",
    "slide_tumor_sublabels = slide_tumor_sublabels[0:n_tumor//10]\n",
    "\n",
    "slide_sublabels = []\n",
    "\n",
    "slide_sublabels.extend(slide_tumor_sublabels)\n",
    "slide_sublabels.extend(slide_healthy_sublabels)\n",
    "\n",
    "print(len(slide_tumor_subimages), len(slide_healthy_subimages))\n",
    "print(len(slide_healthy_sublabels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "VzgJU1QMiuxh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images for  :  (16034, 150, 150, 3)\n",
      "Labels:  (16034,)\n"
     ]
    }
   ],
   "source": [
    "slide_sublabels = np.array(slide_sublabels)\n",
    "dataset = []\n",
    "\n",
    "dataset_slide_subimages = np.vstack((slide_tumor_subimages,slide_healthy_subimages))\n",
    "\n",
    "print(\"Images for  : \", dataset_slide_subimages.shape)\n",
    "print(\"Labels: \", slide_sublabels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Save datasets for later loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('train_subimages' + str(level), dataset_slide_subimages)\n",
    "np.save('train_labels' + str(level), slide_sublabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2cIGe2X8hCsv"
   },
   "source": [
    "# With Data Augmentation\n",
    "\n",
    "To further make the set balanced, we will carry out transformations on tumor images and augment the data with them. \n",
    "The basic augmentations we try are -\n",
    "1. flip horizontal\n",
    "2. flip vertical\n",
    "3. flip horizontal and vertical \n",
    "\n",
    "This leads to 4x times the tumor image dataset collection!\n",
    "\n",
    "We also check to see if we have enough samples already. In that case (>15000 samples), we do not run augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "uDnzipsVhGZi"
   },
   "outputs": [],
   "source": [
    "n_tumor = len(slide_tumor_subimages)\n",
    "augmented = slide_tumor_subimages\n",
    "if(n_tumor <15000):\n",
    "\n",
    "    i=0\n",
    "    for image in slide_tumor_subimages:\n",
    "      i+=1\n",
    "\n",
    "      lr = np.fliplr(image)  \n",
    "      ud = np.flipud(image)\n",
    "      lrud = np.flipud(lr) \n",
    "\n",
    "      if i<=5:\n",
    "        fig, axes = plt.subplots(1,4, figsize=(20,5))\n",
    "        a = axes[0]\n",
    "        a.imshow(image)\n",
    "        a.grid(False)\n",
    "        a = axes[1]\n",
    "        a.imshow(lr)\n",
    "        a.grid(False)\n",
    "        a = axes[2]\n",
    "        a.imshow(ud)\n",
    "        a.grid(False)\n",
    "        a = axes[3]\n",
    "        a.imshow(lrud)\n",
    "        a.grid(False)\n",
    "\n",
    "      image = np.expand_dims(image,axis = 0)\n",
    "      #print(image.shape)\n",
    "      lr = np.expand_dims(lr,axis = 0)\n",
    "      #print(lr.shape)\n",
    "      ud = np.expand_dims(ud,axis = 0)\n",
    "      #print(ud.shape)\n",
    "      lrud = np.expand_dims(lrud,axis = 0)\n",
    "      #print(lrud.shape)\n",
    "      augmented = np.vstack((augmented,ud))\n",
    "      augmented = np.vstack((augmented,lr))\n",
    "      augmented = np.vstack((augmented,lrud))\n",
    "\n",
    "    print(\"Augmented dataset \", augmented.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_SqGDcaRjrG4"
   },
   "source": [
    "Next, we drop images from healthy set to almost balance this skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "xZlozygHjqZo"
   },
   "outputs": [],
   "source": [
    "n_tumor = len(augmented)\n",
    "n_h = n_healthy\n",
    "if((n_h+200) < n_tumor):\n",
    "    n_tumor = n_h\n",
    "else:\n",
    "    n_h = n_tumor + 200 \n",
    "\n",
    "slide_healthy_subimages_augmented = slide_healthy_subimages[0:n_h,:]\n",
    "\n",
    "randomize = np.arange(n_tumor)\n",
    "np.random.shuffle(randomize)\n",
    "augmented = augmented[randomize]\n",
    "augmented = augmented[1:n_tumor,:]\n",
    "\n",
    "augmented_dataset_images = np.vstack((augmented,slide_healthy_subimages_augmented))\n",
    "augmented_dataset_labels = []\n",
    "\n",
    "augmented_dataset_labels.extend([1 for i in range(len(augmented))])\n",
    "augmented_dataset_labels.extend([0 for i in range(len(slide_healthy_subimages_augmented))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "bqwvB4ymlAQt"
   },
   "outputs": [],
   "source": [
    "print(\"New tumor images  : \", augmented.shape)\n",
    "print(\"New healthy images: \", slide_healthy_subimages_augmented.shape)\n",
    "\n",
    "augmented_dataset_labels = np.array(augmented_dataset_labels)\n",
    "print(\"Labels: \", len(augmented_dataset_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TPQkWWjlwuR4"
   },
   "source": [
    "# **Save datasets for later loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "_hJld9xAkzx_"
   },
   "outputs": [],
   "source": [
    "np.save('train_subimages_augmented' + str(level), augmented_dataset_images)\n",
    "np.save('train_labels_augmented' + str(level), augmented_dataset_labels)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Applied deep learning project - single scale.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
